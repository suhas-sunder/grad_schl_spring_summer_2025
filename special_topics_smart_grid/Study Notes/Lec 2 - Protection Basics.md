### Before Smart Grid ![[Pasted image 20250510191338.png]]
- Before smart grid started, we had protection monitoring control, but not at the same level as Smart Grid, but some portions were there. Some data could be transferred between devices because there was some communication. So that date could be taken to management and transaction processing. Not really like smart grid but still somewhat capable. Some of that data could be taken do design and analysis too. Again, not at the same level as Smart Grid but some aspects were there. The main emphasis was on these digital devices that could do protection, monitoring and control. Those are the devices we'll be talking mostly on this course which can be linked to Smart Grid. 
### System faults
- **Types of system faults**: ![[Pasted image 20250510191427.png]] Majority of the power system elements are exposed to the environment such as substations, transmission lines, etc. that are sitting outside. As a result, they are exposed to the elements of weather. Electrons have to be confined to a conductor. As soon as they aren't, it creates a fault. So for eg. if lightning strikes a line, it connects to the ground, so flow of electrons goes to the ground. A fault. If two conductors get connected instead, that's also a fault. You have a conductor and an insulator above the line. If the insulator gets contaminated then that's a fault because the electrons are not confined to the conductor itself. Animals can get into the distribution substations and cause a fault. Human error could occur, eg. maintenance tech leaves a tool where it shouldn't be, and when system turns on, there is a fault. Trees falling on lines. Trees may touch conductors when they sag on really hot days causing a fault. Conductors and insulators can break down faster due to age and elements combined.
- **What happens during a fault**: ![[Pasted image 20250510191451.png]] Phase A, b, c, and neutral are shown in the diagram slide titled "Voltages and currents during faults". The graph starts by showing normal current close to 200 amps. As soon as lightning strikes, phase B current goes close to 4500 amps. A and C doesn't change. Neutral goes up in the opposite direction because everything has to add to zero. So it goes down to around 4500. There is a decaying DC component in phase B because the peaks are going down over time after the spike. So the magnitude goes up and there is a decaying DC component in phase B. There may be some high frequency components there because of the fluctuating waves. Majority of it is 60hz. Same thing happens with neutral current because it is the opposite.
- Then after 2.5 cycles of 60hz . One cycle is 1/60 seconds or 16.67ms. Therefore 16.67 times 2.5 so around 40ms is how long phase B was disconnected. The protection device detected the lightning strike within that time, and fixed the issue as shown in the graph. So it has to be extremely fast, including the breaker time. The device has to detect the issue and give a command to the breaker to open which phase is out of sync. So the device should be a few ms only so that you can account the time required to open the breaker. Some breakers can take up to 2.5 to 3 cycles to open, so the device has to be very quick. Speed is very important.
- Speed and accuracy in DSP are opposite to each other. If you want accurate result, it will be very slow. If you want fast results it won't be as accurate. This is why this area of science is called Art and Science, where getting the right balance is the Art part. The normal current was about 200 ampere. During the strike it went up to around 1400 ampere during the strike. So the range of measuring amperes should be 0 to 5000 amperes. This is a challenge because you have to be able to measure this dynamic range, and be accurate through this range to maintain that.
- **What happens to the voltage during the fault**: ![[Pasted image 20250510191508.png]] Looking at the same fault as previous slide. The voltage goes down for phase B, but the other two volts are consistent. Notice how there is a lot of notice, which is due to the high frequency components of a lightning strike. Voltage is generally less of a problem to measure because the range is much smaller. It goes down, but not by like 20 times which the current does. So when the fault happens and when it clears (breaker kicks in) there is noise.
- Main purpose to develop algorithms is to extract certain frequency components from waveforms. Sometimes we want to extract harmonics. It depends on the application. Same for voltage, certain components will extracted.
- **Requirements for protection is speed, selectivity ![[Pasted image 20250510191537.png]]
	- Selectivity in the sense that when a fault happens anywhere in the system, we measure the current only at that particular line, but current in other lines also go up, because generators are generating that current, which supply current to other lines in the area. So the protection device has to selectively target the line to be remove/isolated so that it isn't affecting other lines unnecessarily.
	- Speed is important because if the fault stays for too long, lines could burn, transformers could burn up, other damage could occur. The system also becomes unstable if the fault is left beyond the critical clearing time (every system has this) the system becomes unstable. NERC will then kick you out because you are not able to remain connected to the system. The large amounts of current will create so much mechanical force that it will structurally destroy wires etc.

### Relays
- Relays will continuously measure the voltage and current and might be placed on either side of a transmission line etc. Current transformers are used to lower the current, generally to 1amperes or 5 amperes which is the nominal value. Similarly, if the voltage is 500, then it will be lowered to 140 volt. The scale goes down but everything else stays the same, the shape of the waves, etc. and that is where it will sit. So a current spike to 4500 might get reduce by a scale of 50 to 1. Relays have become a fundamental part of smart grid because it is already measuring voltages and currents which smart grid needs at every point in the system. Relays have memory too, so it can store the values, which is how we have the graph in the lecture notes. It stored the data during the lightning strike which became useful for analysis, which is rare since faults rarely happen. Relays monitor the system at every 1/4800 seconds. 
- **Zones of protection**: ![[Pasted image 20250510191639.png]]The device is split up into different zones of protection. Line protection zone, busbar protection. Generator protection. Motor protection. The devices are responsible for protecting that zone. The algorithm only measures the activities in that area, so that that device knows that when something goes wrong, it's either happening in that area or somewhere else.
- If a device becomes more secure, it becomes less dependable. If it becomes less secure, it becomes more dependable.
- **Evolution of relays**: ![[Pasted image 20250510191716.png]] Fuse sees overcurrent and blows if it goes over. Then Bi-metallic doesn't blow but opens up instead. Then Electromechanical relays used the principals of electrical systems which would operate depending on mechanical principals based on how it was applied. Then static relays which are mainly operational amplifiers which didn't last very long because they didn't have much reliability. Reliability has to be very high because faults happen very rarely, but when it happens, it has to be extremely reliable when it is needed. Then digital relays were developed which used digital circuits. Then numerical relays (microprocessor) which had communication, memory, self supervision. As we started moving towards smart grid they started adding intelligent functions, non-fault monitoring, integration and automation, flexibility. Next level is IED and more.
- Prodar 70 system was the size of a room because programming was fairly new at the time, but the computations and computing they did was very complex. It was a research project, and wasn't a commercial product to show the capabilities. Even if they wanted to make a  commercial product they couldn't due to the massive size of the computer. Based on analyzing the voltage and current they could determine the exact location of the fault using complex algorithms. These features are very common now where it can self monitor and report that something is going on. Back then it was a very expensive experiment which lead the way to what we have these days. it was a great proof of concept, and there were other experiments too after that.
- DSP methods (voice processing, communication, etc.) have vast amounts of time available, but we have extremely limited time for fault detection, so the methods are different from DSP. Also the data available is very limited compared to the traditional DSP methods which have vast amounts of data available like voice samples etc. A lot of computation is involved and phasors are involved. 
- Discrete Fourier transform was used for a long time and is still the most widely used phasor technique but better techniques are available these days.
- Time domain methods don't calculate the phasors but provide the same results, which is good because phasor calculation can be very complicated or time consuming. 
- Measuring frequency in a very short time is even more challenging than measuring phasers.
- Back then they couldn't even do simple calculations like multiplication using processors so they had to use things like True RMS values etc. In the mid 80's simple microprocessors were invented which allowed for a bit more complexity.
- A lot of startup companies fail because no one wants a product that doesn't save money. The cost is too high, or the innovation is ahead of it's time. That's why microprocessor relays became popular, due to the cost savings, functionality, and delivery time. Spares had to be kept in the past because delivery times could take up to a year. With microprocessor relays, it was easy to manufacture and ship within a few weeks time, so no inventory needed. ![[Pasted image 20250510191811.png]]
- General purpose or universal relays are being developed where the hardware is common.
- You can now put many functionalities in one unit, where in the past it had to be separated into different units. These are called multifunctional products. Instead of wiring ten units, you're wiring only one. These save space and burden (3 ampere at 125 vdc vs 100mA at 124 vdc). 


### History ![[Pasted image 20250510192219.png]] ![[Pasted image 20250510192231.png]] ![[Pasted image 20250510192242.png]] ![[Pasted image 20250510192302.png]]

### Microprocessor Relays ![[Pasted image 20250510192403.png]] ![[Pasted image 20250510192419.png]] ![[Pasted image 20250510192446.png]] ![[Pasted image 20250510192503.png]] ![[Pasted image 20250510192518.png]] ![[Pasted image 20250510192533.png]] ![[Pasted image 20250510192552.png]] ![[Pasted image 20250510192611.png]]
### Issues with existing Relays ![[Pasted image 20250510192623.png]]
- Since relays sit in one spot constantly monitoring the zone without communicating externally, it was hard to tell whether the relays had died. It was very expensive to do maintenance work to check if it working or not, and also deciding how often it had to be maintained was difficult to determine. the newer relays report failures instantly, self calibrate, no maintenance required due to self checking, and repairing is extremely easy since the system is modular and can be easily swapped out. Can also load settings remotely.
- Issues: 
	- Short lifecycle because these are computer based, so their lifecycle is very short. You can't change everything every five years like we do with computers that update constantly.
	- Maintenance people have to be trained each time technology changes.
	- More susceptible to surges, transients, etc. since they are microprocessor based compared to electromechanical devices, though they are shieled from electromagnetic interferences etc.
	- The testing procedures are completely different which increases the possibilities of the number of errors. Even now, tests developed are not standardized and are still an issue.

### Block Diagram ![[Pasted image 20250510192659.png]]
- This is a typical microprocess array (IED: integrated electronic device). The voltage and current does not change waveform, so it stays the same, but it lowers the level. So it brings it down to 1 or 5 amperes (nominal meaning normal current). So 500 ampere will become 1 ampere, where the ratio is 500 to 1. So 5000 will become 1 ampere. Voltage will go from 230 to 110 or 130v, but not change the shape of current or voltage. Then the voltage and current will be brought into analog input subsystem. This will convert the current, lets say 10 amperes during the fault, into voltage because microprocessor systems only work with voltages not currents. That's why this occurs in the analog input systems. It has to be brought down to either 5 or 10 volts, which is done by auxiliary transformers within the analog input subsystems. Then analog filters are used to filter out unwanted frequency components. 
-  Nyquist criterion says if you are sampling at 720 hz, you should filter your signal before sampling in such a way that it doesn't contain, or you remove all frequencies above 1/2 of the frequency in question. Therefore anything above 360 Hz needs to be removed, which is the theoretical limit. In reality it might be even lower. If this isn't done, then we have aliasing, where high frequency components start showing up as low frequency components. eg. 400Hz is included but it won't show up as 400Hz, corrupting the results because the Nyquist criterion is not being followed. This is handled in the analog input subsystem to filter out unwanted frequencies. It reduces the level, converts the current to equivalent voltage, then filters so that we meet the Nyquist criterion so that there is no aliasing. Those are the fundamental components. It also samples at a particular frequency and converts all these sampled values to digital numbers. So if we're sampling 720 hz, in one sample we get 720/60 hz which is 12 samples. So all those values are converted to digital numbers as time goes on, and those digital numbers are brought into the microprocessor. Then the algorithm is implemented at this stage.
- **Digital input subsystems**: Comes in as binary 0's and 1's. Then it goes into microprocessors (multiple chips) which have memory, communication ports, etc. The communication part has multiple ports including ethernet ports, etc. which allows for various logical nodes (multifunction devices)

### Data Flow ![[Pasted image 20250510192800.png]] ![[Pasted image 20250510192833.png]] ![[Pasted image 20250510192903.png]] ![[Pasted image 20250510192919.png]] ![[Pasted image 20250510192934.png]]
- From the power system we get voltage current signals. From CT/PT we reduce the voltage levels. Then it goes into ADC, and you sample them at a specific hz. modern systems do it at 9600 Hz. It is then converted to samples and converted to digital numbers. Then the algorithms are applied in the processor.
- **Window**: A window of samples is taken to do the calculation, for example (three sample window) three data points to do the calculations and make a decision. As we move along in time, the next sample comes in. The previous sample is dropped and the next sample is taken into account, so the window shifts. The same calculations are performed at the target speed, eg. 9600 Hz. This is what IDE's do constantly.